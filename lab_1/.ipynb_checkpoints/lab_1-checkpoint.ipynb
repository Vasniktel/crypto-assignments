{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "import math\n",
    "import random as rand\n",
    "\n",
    "rand.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Decrypt the problem statement\n",
    "Solution: use base64 twice to get the plaintext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key is `23`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now try a repeating-key XOR cip9er. E.g. it s9ould take a string \"9ello world\" and, given t9e key is \"key\", xor t9e first letter \"9\" wit9 \"k\", t9en xor \"e\" wit9 \"e\", t9en \"l\" wit9 \"y\", and t9en xor next c9ar \"l\" wit9 \"k\" again, t9en \"o\" wit9 \"e\" and so on. You may use an index of coincidence, Hamming distance, \\x1csiski examination, statistical tests or w9atever met9od you feel would s9ow t9e best result.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(s):\n",
    "    result = []\n",
    "\n",
    "    for x in range(256):\n",
    "        result.append(''.join(chr(ord(c) ^ x) for c in s))\n",
    "    \n",
    "    return result\n",
    "\n",
    "data = 'Yx`7cen7v7ergrvc~yp:|rn7OXE7t~g.re97R9p97~c7d.xb{s7cv|r7v7dce~yp75.r{{x7`xe{s57vys;7p~ary7c.r7|rn7~d75|rn5;7oxe7c.r7q~edc7{rccre75.57`~c.75|5;7c.ry7oxe75r57`~c.75r5;7c.ry75{57`~c.75n5;7vys7c.ry7oxe7yroc7t.ve75{57`~c.75|57vpv~y;7c.ry75x57`~c.75r57vys7dx7xy97Nxb7zvn7bdr7vy7~ysro7xq7tx~yt~srytr;7_vzz~yp7s~dcvytr;7\\vd~d|~7rovz~yvc~xy;7dcvc~dc~tv{7crdcd7xe7`.vcrare7zrc.xs7nxb7qrr{7`xb{s7d.x`7c.r7urdc7erdb{c9'\n",
    "result = decode(data)[23]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key is `K3k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key size is 3\n"
     ]
    }
   ],
   "source": [
    "# The ciphertext is a hex - try to get the original cipher\n",
    "data = '1c41023f564b2a130824570e6b47046b521f3f5208201318245e0e6b40022643072e13183e51183f5a1f3e4702245d4b285a1b23561965133f2413192e571e28564b3f5b0e6b50042643072e4b023f4a4b24554b3f5b0238130425564b3c564b3c5a0727131e38564b245d0732131e3b430e39500a38564b27561f3f5619381f4b385c4b3f5b0e6b580e32401b2a500e6b5a186b5c05274a4b79054a6b67046b540e3f131f235a186b5c052e13192254033f130a3e470426521f22500a275f126b4a043e131c225f076b431924510a295f126b5d0e2e574b3f5c4b3e400e6b400426564b385c193f13042d130c2e5d0e3f5a086b52072c5c192247032613433c5b02285b4b3c5c1920560f6b47032e13092e401f6b5f0a38474b32560a391a476b40022646072a470e2f130a255d0e2a5f0225544b24414b2c410a2f5a0e25474b2f56182856053f1d4b185619225c1e385f1267131c395a1f2e13023f13192254033f13052444476b4a043e131c225f076b5d0e2e574b22474b3f5c4b2f56082243032e414b3f5b0e6b5d0e33474b245d0e6b52186b440e275f456b710e2a414b225d4b265a052f1f4b3f5b0e395689cbaa186b5d046b401b2a500e381d4b23471f3b4051641c0f2450186554042454072e1d08245e442f5c083e5e0e2547442f1c5a0a64123c503e027e040c413428592406521a21420e184a2a32492072000228622e7f64467d512f0e7f0d1a'\n",
    "data = bytes.fromhex(data)\n",
    "\n",
    "subs = defaultdict(list)\n",
    "\n",
    "NGRAMS_LENGTHS_BOUNDS = (3, 20)\n",
    "\n",
    "# An implementation of Kasiski examination.\n",
    "#\n",
    "# Compute indices of all substrings with sizes in `NGRAMS_LENGTHS_BOUNDS`.\n",
    "for l in range(*NGRAMS_LENGTHS_BOUNDS):\n",
    "    for start in range(len(data) - l + 1):\n",
    "        subs[tuple(data[start:start + l])].append(start)\n",
    "\n",
    "# Remove all substrings with size 1.\n",
    "for key in list(subs.keys()):\n",
    "    if len(subs[key]) == 1:\n",
    "        del subs[key]\n",
    "\n",
    "# Compute the length of the key using GCD over all distances between substrings.\n",
    "diffs = set()\n",
    "for indices in subs.values():\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(i + 1, len(indices)):\n",
    "            diffs.add(indices[j] - indices[i])\n",
    "\n",
    "gcd = diffs.pop()\n",
    "for diff in diffs:\n",
    "    gcd = math.gcd(gcd, diff)\n",
    "\n",
    "print('Key size is', gcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(107, 26), (19, 22), (75, 30)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "KEY_LENGTH = 3\n",
    "\n",
    "segments = [data[i::KEY_LENGTH] for i in range(KEY_LENGTH)]\n",
    "freqs = [Counter(el).most_common(1)[0] for el in segments]\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key is K3k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write a code to attack some simple substitution cipher. To reduce the complexity of this one we will use only uppercase letters, so the keyspace is only 26! To get this one right automatically you will probably need to use some sort of genetic algorithm (which worked the best last year), simulated annealing or gradient descent. Seriously, write it right now, you will need it to decipher the next one as well. Bear in mind, thereÃ¢\\x80\\x99s no spaces. https://docs.google.com/document/d/1AWywcUIMoGr_cjOMaqjqeSyAyzK93icQE4W-6bDELfQ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(s, key):\n",
    "    result = []\n",
    "    i = 0\n",
    "    for c in s:\n",
    "        result.append(chr(c ^ key[i]))\n",
    "        i = (i + 1) % len(key)\n",
    "    return ''.join(result)\n",
    "\n",
    "key = [el ^ ord(' ') for el, _ in freqs]\n",
    "print('key is', ''.join(chr(el) for el in key))\n",
    "decode(data, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key is `UWYGADFPVZBECKMTHXSLRINQOJ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'EFFPQLEKVTVPCPYFLMVHQLUEWCNVWFYGHYTCETHQEKLPVMSAKSPVPAPVYWMVHQLUSPQLYWLASLFVWPQLMVHQLUPLRPSQLULQESPBLWPCSVRVWFLHLWFLWPUEWFYOTCMQYSLWOYWYETHQEKLPVMSAKSPVPAPVYWHEPPLUWSGYULEMQTLPPLUGUYOLWDTVSQETHQEKLPVPVSMTLEUPQEPCYAMEWWYTYWDLUULTCYWPQLSEOLSVOHTLUYAPVWLYGDALSSVWDPQLNLCKCLRQEASPVILSLEUMQBQVMQCYAHUYKEKTCASLFPYFLMVHQLUPQLHULIVYASHEUEDUEHQBVTTPQLVWFLRYGMYVWMVFLWMLSPVTTBYUNESESADDLSPVYWCYAMEWPUCPYFVIVFLPQLOLSSEDLVWHEUPSKCPQLWAOKLUYGMQEUEMPLUSVWENLCEWFEHHTCGULXALWMCEWETCSVSPYLEMQYGPQLOMEWCYAGVWFEBECPYASLQVDQLUYUFLUGULXALWMCSPEPVSPVMSBVPQPQVSPCHLYGMVHQLUPQLWLRPOEDVMETBYUFBVTTPENLPYPQLWLRPTEKLWZYCKVPTCSTESQPQULLGYAUMEHVPETFWMEHVPETBZMEHVPETB'\n",
    "\n",
    "\n",
    "def key_to_str(raw_key):\n",
    "    return ''.join(chr(ord('A') + el) for el in raw_key)\n",
    "\n",
    "def compute_ngram_frequency(text, n):\n",
    "    result = defaultdict(int)\n",
    "    for i in range(len(text) - n + 1):\n",
    "        result[text[i:i + n]] += 1\n",
    "    return result\n",
    "\n",
    "def decode_monoalphabet(encoded, raw_key):\n",
    "    mapping = dict(zip(string.ascii_uppercase, key_to_str(raw_key)))\n",
    "    return ''.join(mapping[c] for c in encoded)\n",
    "\n",
    "def get_fitness(encoded, decoder, ngram_len, reference_freqs):\n",
    "    def fitness(raw_key):\n",
    "        decoded = decoder(encoded, raw_key)\n",
    "        result = 0\n",
    "        \n",
    "        for ngram, freq in compute_ngram_frequency(decoded, ngram_len).items():\n",
    "            if ngram in reference_freqs:\n",
    "                result += freq * math.log2(reference_freqs[ngram])\n",
    "\n",
    "        return result\n",
    "    return fitness\n",
    "\n",
    "def read_into_dict(file_name):\n",
    "    result = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            ngram, freq = line.split()\n",
    "            result[ngram] = int(freq)\n",
    "    return result\n",
    "        \n",
    "def generate_monoalphabet_chromosome():\n",
    "    return rand.sample(range(26), 26)\n",
    "\n",
    "def monoalphabet_maybe_mutate(chromosome, mutation_probability):\n",
    "    if len(chromosome) > 1 and rand.random() <= mutation_probability:\n",
    "        chromosome = chromosome.copy()\n",
    "        i, j = rand.sample(range(len(chromosome)), 2)\n",
    "        chromosome[i], chromosome[j] = chromosome[j], chromosome[i]\n",
    "    return chromosome\n",
    "\n",
    "def _do_crossover(a, b, inherited_a):\n",
    "    result = [0] * 26\n",
    "    a_indices = {el: index for index, el in enumerate(a)}\n",
    "    inherited_b = set(range(26)) - set(inherited_a)\n",
    "    indices_inherited_b = set(range(26)) - set(a_indices[el] for el in inherited_a)\n",
    "    \n",
    "    for el in inherited_a:\n",
    "        result[a_indices[el]] = el\n",
    "    \n",
    "    for el in b:\n",
    "        if el in inherited_b:\n",
    "            result[indices_inherited_b.pop()] = el\n",
    "    \n",
    "    assert sorted(result) == list(range(26))\n",
    "    return result\n",
    "\n",
    "def monoalphabet_crossover(a, b):\n",
    "    inherited_from_a = rand.sample(range(26), rand.randint(1, 25))\n",
    "    inherited_from_b = set(range(26)) - set(inherited_from_a)\n",
    "    return _do_crossover(a, b, inherited_from_a), _do_crossover(b, a, inherited_from_b)\n",
    "    \n",
    "def run(\n",
    "    number_of_iterations,\n",
    "    population_size,\n",
    "    surviving_percentage,\n",
    "    mating_percentage,\n",
    "    mutation_probability,\n",
    "    generate_fn,\n",
    "    fitness_fn,\n",
    "    crossover_fn,\n",
    "    mutate_fn,\n",
    "    debug=True\n",
    "):\n",
    "    population = [generate_fn() for _ in range(population_size)]\n",
    "    \n",
    "    for i in range(number_of_iterations):\n",
    "        if debug:\n",
    "            percentage = round(i / number_of_iterations * 100, ndigits=2)\n",
    "            print(f'Progress: {percentage:.2f}%', end='\\r')\n",
    "        \n",
    "        population.sort(key=fitness_fn, reverse=True)\n",
    "        population = population[:population_size]\n",
    "        \n",
    "        new_population = population[:int(population_size * surviving_percentage)]\n",
    "        while len(new_population) < population_size:\n",
    "            a, b = rand.sample(range(int(population_size * mating_percentage)), 2)\n",
    "            children = crossover_fn(population[a], population[b])\n",
    "            new_population += [mutate_fn(el, mutation_probability) for el in children]\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    return sorted(population, key=fitness_fn, reverse=True)\n",
    "\n",
    "\n",
    "NGRAM_LENGTH = 4\n",
    "NGRAM_REFERENCE_FREQS = read_into_dict('english_quadgrams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is UWYGADFPVZBECKMTHXSLRINQOJ\n"
     ]
    }
   ],
   "source": [
    "result = run(\n",
    "    number_of_iterations=500,\n",
    "    population_size=300,\n",
    "    surviving_percentage=0.2,\n",
    "    mating_percentage=0.6,\n",
    "    mutation_probability=0.6,\n",
    "    generate_fn=generate_monoalphabet_chromosome,\n",
    "    crossover_fn=monoalphabet_crossover,\n",
    "    mutate_fn=monoalphabet_maybe_mutate,\n",
    "    fitness_fn=get_fitness(\n",
    "        encoded=data, \n",
    "        decoder=decode_monoalphabet, \n",
    "        ngram_len=NGRAM_LENGTH, \n",
    "        reference_freqs=NGRAM_REFERENCE_FREQS\n",
    "    )\n",
    ")\n",
    "raw_key = result[0]\n",
    "print('Key is', key_to_str(raw_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADDTHEABILITYTODECIPHERANYKINDOFPOLYALPHABETICSUBSTITUTIONCIPHERSTHEONEUSEDINTHECIPHERTEXTSHEREHASTWENTYSIXINDEPENDENTRANDOMLYCHOSENMONOALPHABETICSUBSTITUTIONPATTERNSFOREACHLETTERFROMENGLISHALPHABETITISCLEARTHATYOUCANNOLONGERRELYONTHESAMESIMPLEROUTINEOFGUESSINGTHEKEYBYEXHAUSTIVESEARCHWHICHYOUPROBABLYUSEDTODECIPHERTHEPREVIOUSPARAGRAPHWILLTHEINDEXOFCOINCIDENCESTILLWORKASASUGGESTIONYOUCANTRYTODIVIDETHEMESSAGEINPARTSBYTHENUMBEROFCHARACTERSINAKEYANDAPPLYFREQUENCYANALYSISTOEACHOFTHEMCANYOUFINDAWAYTOUSEHIGHERORDERFREQUENCYSTATISTICSWITHTHISTYPEOFCIPHERTHENEXTMAGICALWORDWILLTAKETOTHENEXTLABENJOYBITLYSLASHTHREEFOURCAPITALDNCAPITALWJCAPITALW'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_monoalphabet(data, raw_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADD THE ABILITY TO DECIPHER ANY KIND OF POLYALPHABETIC SUBSTITUTION CIPHERS. THE ONE USED IN THE CIPHERTEXTS HERE  HAS TWENTY SIX INDEPENDENT RANDOMLY CHOSEN MONOALPHABETIC SUBSTITUTION PATTERNS FOR EACH LETTER FROM ENGLISH  ALPHABET. IT IS CLEAR THAT YOU CAN NO LONGER RELY ON THE SAME SIMPLE ROUTINE OF GUESSING THE KEY BY EXHAUSTIVE  SEARCH, WHICH YOU PROBABLY USED TO DECIPHER THE PREVIOUS PARAGRAPH. WILL THE INDEX OF COINCIDENCE STILL WORK? AS A  SUGGESTION, YOU CAN TRY TO DIVIDE THE MESSAGE IN PARTS BY THE NUMBER OF CHARACTERS IN A KEY AND APPLY FREQUENCY ANALYSIS TO EACH OF THEM. CAN YOU FIND A WAY TO USE HIGHER ORDER FREQUENCY STATISTICS WITH THIS TYPE OF CIPHER? THE NEXT MAGICAL WORD WILL TAKE TO THE NEXT LAB. ENJOY! BIT.LY/34DnWjW'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatted text\n",
    "'''\n",
    "ADD THE ABILITY TO DECIPHER ANY KIND OF POLYALPHABETIC SUBSTITUTION CIPHERS. THE ONE USED IN THE CIPHERTEXTS HERE \n",
    "HAS TWENTY SIX INDEPENDENT RANDOMLY CHOSEN MONOALPHABETIC SUBSTITUTION PATTERNS FOR EACH LETTER FROM ENGLISH \n",
    "ALPHABET. IT IS CLEAR THAT YOU CAN NO LONGER RELY ON THE SAME SIMPLE ROUTINE OF GUESSING THE KEY BY EXHAUSTIVE \n",
    "SEARCH, WHICH YOU PROBABLY USED TO DECIPHER THE PREVIOUS PARAGRAPH. WILL THE INDEX OF COINCIDENCE STILL WORK? AS A \n",
    "SUGGESTION, YOU CAN TRY TO DIVIDE THE MESSAGE IN PARTS BY THE NUMBER OF CHARACTERS IN A KEY AND APPLY FREQUENCY\n",
    "ANALYSIS TO EACH OF THEM. CAN YOU FIND A WAY TO USE HIGHER ORDER FREQUENCY STATISTICS WITH THIS TYPE OF CIPHER?\n",
    "THE NEXT MAGICAL WORD WILL TAKE TO THE NEXT LAB. ENJOY! BIT.LY/34DnWjW\n",
    "'''.strip().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key size is 4 with IC 0.0876424189307625\n"
     ]
    }
   ],
   "source": [
    "data = 'KZBWPFHRAFHMFSNYSMNOZYBYLLLYJFBGZYYYZYEKCJVSACAEFLMAJZQAZYHIJFUNHLCGCINWFIHHHTLNVZLSHSVOZDPYSMNYJXHMNODNHPATXFWGHZPGHCVRWYSNFUSPPETRJSIIZSAAOYLNEENGHYAMAZBYSMNSJRNGZGSEZLNGHTSTJMNSJRESFRPGQPSYFGSWZMBGQFBCCEZTTPOYNIVUJRVSZSCYSEYJWYHUJRVSZSCRNECPFHHZJBUHDHSNNZQKADMGFBPGBZUNVFIGNWLGCWSATVSSWWPGZHNETEBEJFBCZDPYJWOSFDVWOTANCZIHCYIMJSIGFQLYNZZSETSYSEUMHRLAAGSEFUSKBZUEJQVTDZVCFHLAAJSFJSCNFSJKCFBCFSPITQHZJLBMHECNHFHGNZIEWBLGNFMHNMHMFSVPVHSGGMBGCWSEZSZGSEPFQEIMQEZZJIOGPIOMNSSOFWSKCRLAAGSKNEAHBBSKKEVTZSSOHEUTTQYMCPHZJFHGPZQOZHLCFSVYNFYYSEZGNTVRAJVTEMPADZDSVHVYJWHGQFWKTSNYHTSZFYHMAEJMNLNGFQNFZWSKCCJHPEHZZSZGDZDSVHVYJWHGQFWKTSNYHTSZFYHMAEDNJZQAZSCHPYSKXLHMQZNKOIOKHYMKKEIKCGSGYBPHPECKCJJKNISTJJZMHTVRHQSGQMBWHTSPTHSNFQZKPRLYSZDYPEMGZILSDIOGGMNYZVSNHTAYGFBZZYJKQELSJXHGCJLSDTLNEHLYZHVRCJHZTYWAFGSHBZDTNRSESZVNJIVWFIVYSEJHFSLSHTLNQEIKQEASQJVYSEVYSEUYSMBWNSVYXEIKWYSYSEYKPESKNCGRHGSEZLNGHTSIZHSZZHCUJWARNEHZZIWHZDZMADNGPNSYFZUWZSLXJFBCGEANWHSYSEGGNIVPFLUGCEUWTENKCJNVTDPNXEIKWYSYSFHESFPAJSWGTYVSJIOKHRSKPEZMADLSDIVKKWSFHZBGEEATJLBOTDPMCPHHVZNYVZBGZSCHCEZZTWOOJMBYJSCYFRLSZSCYSEVYSEUNHZVHRFBCCZZYSEUGZDCGZDGMHDYNAFNZHTUGJJOEZBLYZDHYSHSGJMWZHWAFTIAAY'\n",
    "\n",
    "\n",
    "def compute_ic(a, b):\n",
    "    assert len(a) == len(b) and a\n",
    "    result = sum(i == j for i, j in zip(a, b))\n",
    "    return result / len(a)\n",
    "\n",
    "def estimate_number_of_alphabets(encoded):\n",
    "    result = []\n",
    "    for i in range(1, len(encoded) // 2):\n",
    "        shifted = encoded[i:] + encoded[:i]\n",
    "        result.append(compute_ic(encoded, shifted))\n",
    "    return result\n",
    "\n",
    "ics = estimate_number_of_alphabets(data)\n",
    "key_size, ic = max(enumerate(ics, start=1), key=lambda x: x[1])\n",
    "print(f'Key size is {key_size} with IC {ic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_polyalphabet(encoded, raw_key):\n",
    "    mapping = [key_to_str(alphabet) for alphabet in raw_key]\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    for c in encoded:\n",
    "        result.append(mapping[i][ord(c) - ord('A')])\n",
    "        i = (i + 1) % len(raw_key)\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "def get_polyalphabet_decoder(population, current_alphabet_index):\n",
    "    def decoder(encoded, raw_key):\n",
    "        best_alphabets = [el[0] for el in population]\n",
    "        best_alphabets[current_alphabet_index] = raw_key\n",
    "        return decode_polyalphabet(encoded, best_alphabets)\n",
    "    return decoder\n",
    "\n",
    "def run_polyalphabet(\n",
    "    number_of_iterations,\n",
    "    population_size,\n",
    "    surviving_percentage,\n",
    "    mating_percentage,\n",
    "    mutation_probability,\n",
    "    num_of_alphabets\n",
    "):\n",
    "    population = []\n",
    "    for _ in range(num_of_alphabets):\n",
    "        alphabet_population = []\n",
    "        for _ in range(population_size):\n",
    "            alphabet_population.append(rand.sample(range(26), 26))\n",
    "        population.append(alphabet_population)\n",
    "    \n",
    "    for i in range(number_of_iterations):\n",
    "        percentage = round(i / number_of_iterations * 100, ndigits=2)\n",
    "        print(f'Progress: {percentage:.2f}%', end='\\r')\n",
    "        \n",
    "        for j in range(num_of_alphabets):\n",
    "            fitness_fn = get_fitness(\n",
    "                encoded=data,\n",
    "                decoder=get_polyalphabet_decoder(population, j),\n",
    "                ngram_len=NGRAM_LENGTH,\n",
    "                reference_freqs=NGRAM_REFERENCE_FREQS\n",
    "            )\n",
    "            \n",
    "            def generate_fn(it):\n",
    "                return lambda: next(it)\n",
    "            \n",
    "            new_population = run(\n",
    "                number_of_iterations=1,\n",
    "                population_size=population_size,\n",
    "                surviving_percentage=surviving_percentage,\n",
    "                mating_percentage=mating_percentage,\n",
    "                mutation_probability=mutation_probability,\n",
    "                generate_fn=generate_fn(iter(population[j])),\n",
    "                crossover_fn=monoalphabet_crossover,\n",
    "                mutate_fn=monoalphabet_maybe_mutate,\n",
    "                fitness_fn=fitness_fn,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            population[j] = new_population[:population_size]\n",
    "    \n",
    "    return [alphabets[0] for alphabets in population]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is ['LFNGWOMTJECQVSYRDKHIZPUBXA', 'QPYLEABRTDZUIVJGFWNHMKCXSO', 'ANDUKZBTCMJIPSHLFVEQROGXYW', 'LXDQCPEOYJAVISWFZUNRBKGMTH']\n"
     ]
    }
   ],
   "source": [
    "raw_key = run_polyalphabet(\n",
    "    number_of_iterations=500,\n",
    "    population_size=300,\n",
    "    surviving_percentage=0.2,\n",
    "    mating_percentage=0.6,\n",
    "    mutation_probability=0.6,\n",
    "    num_of_alphabets=key_size\n",
    ")\n",
    "print('Key is', [key_to_str(el) for el in raw_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONGRATULATIONSTHISWASNTQUITEANEASYTASKANDONLYACOUPLEOFLASTYEARSTUDENTSGOTTOTHISPOINTNOWALLTHISTEXTISJUSTGARBAGETOLETYOUUSESOMEFREQUENCYANALYSISWESETSAILONTHISNEWSEABECAUSETHEREISNEWKNOWLEDGETOBEGAINEDANDNEWRIGHTSTOBEWONANDTHEYJUSTBEWONANDUSEDFORTHEPROGRESSOFALLPEOPLEFORSPACESCIENCELIKENUCLEARSCIENCEANDALLTECHNOLOGYHASNOCONSCIENCEOFITSOWNWHETHERITWILLBECOMEAFORCEFORGOODORILLDEPENDSONMANANDONLYIFTHEUNITEDSTATESOCCUPIESAPOSITIONOFPREEMINENCECANWEHELPDECIDEWHETHERTHISNEWOCEANWILLBEASEAOFPEACEORANEWTERRIFYINGTHEATEROFWARIDONOTSAYTHEWESHOULDORWILLGOUNPROTECTEDAGAINSTTHEHOSTILEMISUSEOFSPACEANYMORETHANWEGOUNPROTECTEDAGAINSTTHEHOSTILEUSEOFLANDORSEABUTIDOSAYTHATSPACECANBEEXPLOREDANDMASTEREDWITHOUTFEEDINGTHEFIRESOFWARWITHOUTREPEATINGTHEMISTAKESTHATMANHASMADEINEXTENDINGHISWRITAROUNDTHISGLOBEOFOURSWECHOOSETOGOTOTHEMOONINTHISDECADEANDDOTHEOTHERTHINGSNOTBECAUSETHEYAREEASYBUTBECAUSETHEYAREHARDBECAUSETHATGOALWILLSERVETOORGANIMEANDMEASURETHEBESTOFOURENERGIESANDSKILLSBECAUSETHATCHALLENGEISONETHATWEAREWILLINGTOACCEPTONEWEAREUNWILLINGTOPOSTPONEANDONEWHICHWEINTENDTOWINANDTHEOTHERSTOOOKANDNOWTHEREALDEALBITLYSLASHTHREEDHCAPITALTTHREEEIGHTCAPITALX'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_polyalphabet(data, raw_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CONGRATULATIONS! THIS WASN'T QUITE AN EASY TASK AND ONLY A COUPLE OF LAST YEAR STUDENTS GOT TO THIS POINT.  NOW, ALL THIS TEXT IS JUST GARBAGE TO LET YOU USE SOME FREQUENCY ANALYSIS. WE SET SAIL ON THIS NEW SEA BECAUSE THERE IS NEW KNOWLEDGE TO BE GAINED AND NEW RIGHTS TO BE WON. AND THEY JUST BE WON. AND USED FOR THE PROGRESS OF  ALL PEOPLE FOR SPACE SCIENCE LIKE NUCLEAR SCIENCE AND ALL TECHNOLOGY HAS NO CONSCIENCE OF ITS OWN. WHETHER IT  WILL BECOME A FORCE FOR GOOD OR ILL DEPENDS ON MAN AND ONLY IF THE UNITED STATES OCCUPIES A POSITION OF  PRE-EMINENCE CAN WE HELP DECIDE WHETHER THIS NEW OCEAN WILL BE A SEA OF PEACE OR A NEW TERRIFYING THEATER OF WAR. I DO NOT SAY THE WE SHOULD OR WILL GO UNPROTECTED AGAINST THE HOSTILE MISUSE OF SPACE ANYMORE THAN WE GO  UNPROTECTED AGAINST THE HOSTILE USE OF LAND OR SEA BUT I DO SAY THAT SPACE CAN BE EXPLORED AND MASTERED WITHOUT FEEDING THE FIRES OF WAR WITHOUT REPEATING THE MISTAKES THAT MAN HAS MADE IN EXTENDING HIS WRIT AROUND THIS  GLOBE OF OURS WE CHOOSE TO GO TO THE MOON IN THIS DECADE AND DO THE OTHER THINGS NOT BECAUSE THEY ARE EASY BUT BECAUSE THEY ARE HARD BECAUSE THAT GOAL WILL SERVE TO ORGANI ME AND MEASURE THE BEST OF OUR ENERGIES AND SKILLS  BECAUSE THAT CHALLENGE IS ONE THAT WE ARE WILLING TO ACCEPT ONE WE ARE UNWILLING TO POSTPONE AND ONE WHICH WE  INTEND TO WIN AND THE OTHERS TOO OK AND NOW THERE ALDEAL BIT.LY/3dhT38X\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CONGRATULATIONS! THIS WASN'T QUITE AN EASY TASK AND ONLY A COUPLE OF LAST YEAR STUDENTS GOT TO THIS POINT. \n",
    "NOW, ALL THIS TEXT IS JUST GARBAGE TO LET YOU USE SOME FREQUENCY ANALYSIS. WE SET SAIL ON THIS NEW SEA BECAUSE\n",
    "THERE IS NEW KNOWLEDGE TO BE GAINED AND NEW RIGHTS TO BE WON. AND THEY JUST BE WON. AND USED FOR THE PROGRESS OF \n",
    "ALL PEOPLE FOR SPACE SCIENCE LIKE NUCLEAR SCIENCE AND ALL TECHNOLOGY HAS NO CONSCIENCE OF ITS OWN. WHETHER IT \n",
    "WILL BECOME A FORCE FOR GOOD OR ILL DEPENDS ON MAN AND ONLY IF THE UNITED STATES OCCUPIES A POSITION OF \n",
    "PRE-EMINENCE CAN WE HELP DECIDE WHETHER THIS NEW OCEAN WILL BE A SEA OF PEACE OR A NEW TERRIFYING THEATER OF WAR.\n",
    "I DO NOT SAY THE WE SHOULD OR WILL GO UNPROTECTED AGAINST THE HOSTILE MISUSE OF SPACE ANYMORE THAN WE GO \n",
    "UNPROTECTED AGAINST THE HOSTILE USE OF LAND OR SEA BUT I DO SAY THAT SPACE CAN BE EXPLORED AND MASTERED WITHOUT\n",
    "FEEDING THE FIRES OF WAR WITHOUT REPEATING THE MISTAKES THAT MAN HAS MADE IN EXTENDING HIS WRIT AROUND THIS \n",
    "GLOBE OF OURS WE CHOOSE TO GO TO THE MOON IN THIS DECADE AND DO THE OTHER THINGS NOT BECAUSE THEY ARE EASY BUT\n",
    "BECAUSE THEY ARE HARD BECAUSE THAT GOAL WILL SERVE TO ORGANI ME AND MEASURE THE BEST OF OUR ENERGIES AND SKILLS \n",
    "BECAUSE THAT CHALLENGE IS ONE THAT WE ARE WILLING TO ACCEPT ONE WE ARE UNWILLING TO POSTPONE AND ONE WHICH WE \n",
    "INTEND TO WIN AND THE OTHERS TOO OK AND NOW THERE ALDEAL BIT.LY/3dhT38X\"\"\".replace('\\n', ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
